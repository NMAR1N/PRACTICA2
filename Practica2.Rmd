---
title: "Practica2"
author: "Julio Takimoto"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PRACTICA 2

## Pregunta 1:

1\. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

El primer paso para realizar tareas de crawling y scraping es poder descargar los datos de la web. Para esto usaremos la capacidad de R y de sus librerías (httr y XML) para descargar webs y almacenarlas en variables que podamos convertir en un formato fácil de analizar (p.e. de HTML a XML)

```{r carga_libreria}        
library(httr)
library(httr2)
library(XML)
library(xml2)
library(dplyr)
library(stringr)
library(rvest)
library(ggplot2)
library(gridExtra)
url <- "https://www.mediawiki.org/wiki/MediaWiki"
```
1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R 
apto para ser tratado.

```{r Pregunta 1}
respuesta <- GET(url)
html_contenido <- content(respuesta, "text")
parsedHtmlContenido <- htmlParse(html_contenido, asText = TRUE)

```

2.  Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como "title"). En las cabeceras web encontramos información como el título, los ficheros de estilo visual, y meta-información como el nombre del autor de la página, una descripción de esta, el tipo de codificación de esta, o palabras clave que indican qué tipo de información contiene la página. Una vez descargada la página, y convertida a un formato analizable (como XML), buscaremos los elementos de tipo "title". P.e. "

```{r Pregunta 2}

titulo <- xpathSApply(parsedHtmlContenido, '//title', xmlValue)

```


3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como "a"), buscando el texto del enlace, así como la URL. Vamos a extraer, usando las funciones de búsqueda XML, todos los enlaces que salen de esta página con tal de listarlos y poder descargarlas más tarde. Sabemos que estos son elementos de tipo "", que tienen el atributo"href" para indicar la URL del enlace. P.e. "[Texto del Enlace](‘enlace’)". Del enlace nos quedaremos con la URL de destino y con el valor del enlace (texto del enlace).


```{r Pregunta 3}

links_text   <- xpathSApply(parsedHtmlContenido,"//a",xmlValue)
links_url <- xpathSApply(parsedHtmlContenido, "//a", xmlGetAttr, 'href')
tablaPrinc <- data.frame("enlaces" = links_text, "URL" = links_url, stringsAsFactors = F)
tablaPrinc$Enlace2 <- str_replace_na(tablaPrinc$enlaces)
tablaPrinc$Enlace2[tablaPrinc$Enlace2 == "NA"] <- NA
tablaPrinc

```

4.  Generar una tabla con cada enlace encontrado, indicando el texto que
    acompaña el enlace, y el número de veces que aparece un enlace con ese
    mismo objetivo.
    
    
```{r Pregunta 4}
tablaPrinc$es_relativo <- grepl("^/|#", tablaPrinc$URL, perl = T)
tablaPrinc <- tablaPrinc %>% mutate(url_absoluta = ifelse(es_relativo, paste("https://www.mediawiki.org", URL, sep = ""), URL))
tablaPrinc <- tablaPrinc %>% add_count(url_absoluta)

```
5.  Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de
    status HTTP al hacer una petición a esa URL).
    
```{r Pregunta 5}
Extraer_Code_status <- function(URL) {  resp <- HEAD(url)
  return(resp$status_code)
}
tablaPrinc$status_code <- sapply(tablaPrinc$url_absoluta, Extraer_Code_status)
print(tablaPrinc)

```
    
PARTE II
1. Un histograma con la frecuencia de aparición de los enlaces, pero separado por 
URLs absolutas (con “http…”) y URLs relativas.
```{r Crear el histograma URL Absolutas y Relativas}

tablaPrinc$Tipo <- ifelse(grepl("^https?://", tablaPrinc$URL), "Absoluta", "Relativa")
histograma <- ggplot(tablaPrinc, aes(x = Tipo)) +
  geom_histogram(stat = "count", fill = "steelblue", color = "black") +
  labs(title = "Frecuencia de Enlaces Absolutos y Relativos", x = "Tipo de URL", y = "Frecuencia") + theme_minimal()

print(histograma)

```
2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros 
dominios o servicios vs. la suma de los otros enlaces

```{r Suma de enlaces que apuntan a mediawiki vs. otros}
tablaPrinc$Destino <- ifelse(grepl("^https://www.mediawiki.org", tablaPrinc$URL) | tablaPrinc$Tipo == "Relativa", "MediaWiki", "Otros")

barrasP2 <- ggplot(tablaPrinc, aes(x = Destino)) +
  geom_bar(fill = "lightgreen", color = "black") +
  labs(title = "Enlaces a MediaWiki vs Otros", x = "Destino", y = "Número de Enlaces") +
  theme_minimal()

print(barrasP2)

```

3. Grafico de torta

```{r Pregunta 3}
estado_frecuencia <- table(tablaPrinc$status_code)
estado_df <- as.data.frame(estado_frecuencia)
colnames(estado_df) <- c("Estado", "Frecuencia")

Ptorta <- ggplot(estado_df, aes(x = "", y = Frecuencia, fill = as.factor(Estado))) +
  geom_bar(width = 1, stat = "identity") +
  coord_polar("y", start = 0) +
  labs(title = "Porcentaje de Status de Enlaces", fill = "Estado") +
  theme_void()

print(Ptorta)

```

